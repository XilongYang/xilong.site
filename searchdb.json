{"posts": [{"title": " The Binary Representation of Floating-Point Numbers (IEEE 754)", "url": "/posts/The_Binary_Representation_of_Float_Numbers_IEEE_754.html", "content": "  The binary representation of the floating numbers was makes me very confused many years ago. Here is a introduction to the standard IEEE 754.    Contents   Issue  Analysis Normalized Values  Denormalized Values  Special Values  Precision and Rounding   Result     Issue  Code for the issue:  #include <stdio.h>  int main() {   int arr[10] = {3, 3, 3, 3, 3, 3, 3, 3, 3, 3};    // Calculate the mean of all the numbers in arr.   float a = 0;   for (int i = 0; i < 10; ++i) {     a += (float)arr[i] / 10;   }   for (int i = 0; i < 10; ++i) {     if (arr[i] > a) {         printf(\"%d \", arr[i]);     }   }   return 0; }  It’s evident that the above program theoretically shouldn’t output any data. However, the actual execution result is as follows:  3 3 3 3 3 3 3 3 3 3  Analysis  After several attempts, I finally found that the issue is caused by the value of ‘a’ in this program not being 3.0, but rather 2.9999. This suggests that the problem is probably linked to the precision of the floating-point arithmetic.  Here is a introduction to the IEEE 754 floating-point number standard, which is followed by the C programming language.  A floating-point number which according to the IEEE 754 standard has a form comprising a single sign bit , followed by k bits for the exponent , and n bits for the fraction .  For example, when k = 8 and n = 23, the form is shown in the diagram below,     Sign  Exponent  Fraction      x  xxxx xxxx  xxx xxxx xxxx xxxx xxxx xxxx     The sign bit determines whether the number is positive or negative. It will be set to ‘0’ when the number is positive. Otherwise it will be set to ‘1’.  The k bits in exponent area determine one of three categories for a floating-point number and represents its exponent. Furthermore, the value of k also determines a Bias value calculated as \\(2^{k-1}-1\\) .  The n bits in fraction area determine a value of the number without exponent. Just like the coefficient of a number expressed in scientific notation.  Thus, the value of a floating-point number can be calculated by the expression:  \\[ V = (-1)^S \\times M \\times 2^E \\]  Where S is the value of the sign bit , M is the value represented by the fraction and E is the value represented by the exponent .  Normalized Values  When the k bits in the exponent area are neither all 0s nor all 1s, the number is a normalized value. For a normalized value, the exponent of the number is calculated by the following expression.  \\[ E = e - Bias. \\]  Where e is the unsigned number value of exponent area.  In this form, the fraction area has a implicit leadding 1 in the left of the point. That is:  \\[ M = 1.fraction \\]  For example, a float-point number with k = 3 bits for exponent and n = 4 bits for fraction, which has a bit-level representation 0 001 1010 . It will yield a value:  \\[ S = 0 \\]  \\[ Bias = 2^{k-1} - 1 = 3 \\]  \\[ E = 1 - Bias = -2 \\]  \\[ M = 1.1010 \\]  \\[ V = -1^0 \\times 1.1010 \\times 2^{-2} = 0.011010 \\]   Why don’t we use the exponent value directly rather than minus a suspicious Bias?  The reason is to represent the negative exponent naturally. We can easily compare two exponent just by compare its unsigned value of the bit-level representation.   Denormalized Values  When the bits in the exponent area are all 0s, the number is a denormalized value. There are only 2 difference between a normalized value and a denormalized value.   The exponent of the number is calculated by the following expression.   \\[ E = 1 - Bias \\]   The fraction has no more implicit 1 in the head. That is:   \\[ M = 0.fraction \\]  For example, a float-point number with k = 3 bits for exponent and n = 4 bits for fraction, which has a bit-level representation 0 00 1010 will yield a value:  \\[ S = 0 \\]  \\[ Bias = 2^{k-1} - 1 = 3 \\]  \\[ E = 1 - Bias = -2 \\]  \\[ M = 0.1010 \\]  \\[ V = -1^0 \\times 0.1010 \\times 2^{-2} = 0.001010 \\]   Why don’t we use the \\(-Bias\\) to be the value of the exponent, rather than \\(1-Bias\\) ?  The reason is to take a naturally transform from denormalized values to normalized values.  For example, consider a number which has k = 3 bit to represent the exponent and n = 4 bit for the fraction. The biggest denormalized values in the form has a bit-level represention:  0 000 1111, the values is: \\(0.1111 \\times 2^{1-(2^{3-1}-1)}\\) = \\(0.001111\\)  Increase it by 1 in the bit-level, we can get the smallest normalized number which has a bit-level represention:  0 001 0000, the values is: \\(1.0000 \\times 2^{1-(2^{3-1}-1)}\\) = \\(0.010000\\)  If the exponent set to \\(-Bias\\) directly, the value of the denormalized number will be:  \\(0.1111 \\times 2^{-(2^{3-1} - 1)} =  0.0001111\\)  We can look the \\(1-Bias\\) as \\(-Bias + 1\\) , it is a compensation for the lack of the leading 1 in a denormalized value.   Special Values  When the bits in the exponent area are set to all 1s, there are 2 special form depending on whether the bits in the fraction area are set to all 0s.   When the fraction area are not all 0s, the value is NaN which means Not a Number.  When the fraction area are all 0s, the value is infinity. The value is either \\(+\\infty\\) or \\(-\\infty\\) denpending on the sign bit.   Here a some examples when k = 2 and n = 5:     Bit-level representaion  Value      0 11 00000  \\(+\\infty\\)    1 11 00000  \\(-\\infty\\)    0 11 00100  NaN    1 11 00100  NaN     Precision and Rounding  The C programming language is using 32 bits to represent a float type，and 64 bits to represent a double type. Here is the detail for the representation.     Type  Sign  Exponent  Fraction      float  1 bit  8 bit  23 bit    double  1 bit  11 bit  52 bit     Limited by the memory space, there are 2 factors that can lead a lack of precision.   The number is so large that the exponent can not be represent. For example, the number \\(2^{5000}\\) can not be represented even by the type double , because a double can only represents a exponent between \\(1 - 2^{11 - 1} + 1 = -1022\\) and \\(2^{12} - 2 - 2^{11 - 1} + 1 =1023\\) .  The number has too many digit so that the fraction bits is not enough to represent it. For example, the number \\(0.1100110011001100110011000101_2\\) needs 27 bit to represent its fraction (the leading 1 can be left out), but a float only has 23 bits to represent the fraction.    Note  The effection of the exponent is to move the point to difference posiiton of a floating-point number (that’s why it is called “floating-point”), that makes it possible to get a very large value. But since we can only set the value for a limited fraction, the precision of the possible value is also limited.  For example, when k = 8 and n = 3 we can simply represent \\(2^{100}\\) by the represention:  0 11100011 000  But we can’t represent \\(1.1111_2 \\times 2^{100}\\) since we can only control the first 3 bits actually in the hundred of 0s.   When we face to the precision problem, the only way we can choose is make it rounding. The default rule of rounding is called “Round-to-even”.  To explain the rule, consider a number which has a form like \\(...xxx.xxyyyy...\\) . The position we want to round is between the least x and the most y. A value is on halfway between two possibilities only if it has a form like \\(xxx.xx1000...\\) , that is the most y is 1 and followed by all 0s.   If the value is not on the halfway between two possibilities, round to the nearer one. For example, if we want to save 2 digit after the point, the number 1.01101 will round to 1.10 and the number 1.01001 will round to 1.01.  If the value is on the halfway between two possibilities, we tend to make the least digit before the position we want to round to 0. For example, if we want to save 2 digit after the point, the number 1.01100 will round to 1.10 and the number 1.10100 will round to 1.10.   Because the last digit of a rounded number is always 0 (so that the number is even), the rule is called “round-to-even”.   Why it choose round-to-even instead round-to-zero?  Because a half of numbers is even, a number will round upward about 50% of the time and round downward about 50% of the time. It can balance the loss which caused by rounding.   Result  Let’s back to the issue, get the IEEE 754 representation of \\(0.3_{10}\\) .  Transfer \\(0.3_{10}\\) to normalized binary representation:  \\[ 1.00110011001100110011001... \\times 10_2^{-10} \\]  We can notice that the binary representation of \\(0.3_{10}\\) is a unfiniate number. So it will be rounding when transfer to IEEE 754 representation. Since it is bigger than halfway, it will round upward:     Sign  EXP  Fraction      0  01111101  001 1001 1001 1001 1001 1010     Translate the binary representation to hex, it should be \\(3e99999a_{16}\\) . We can validate it by the following program:  #include<stdio.h> int main() {     float a = 0.3;     printf(\"%x\", *(int *)&a);     return 0; }  output:  3e99999a  The result is met our expectations. "}, {"title": " Some Tricks at the Bit-level", "url": "/posts/Some_Tricks_at_the_Bit_Level.html", "content": "  On my journey through Chapter 2 of CSAPP, some magical tricks appeared intermittently. So I am trying to catch them by writing this article.    Contents   Fold bits  Count bits     Fold bits  Consider that we get a mission to check if all the odd-numbered bits in a int are set to 1. What is the faster way?  The answer is: to fold it.  /*  * allOddBits - return 1 if all odd-numbered bits in word set to 1  *   where bits are numbered from 0 (least significant) to 31 (most significant)  *   Examples allOddBits(0xFFFFFFFD) = 0, allOddBits(0xAAAAAAAA) = 1  *   Legal ops: ! ~ & ^ | + << >>  *   Max ops: 12  *   Rating: 2  */ int allOddBits(int x) {   x = x & (x >> 16);   x = x & (x >> 8);   x = x & (x >> 4);   x = x & (x >> 2);   return (x >> 1) & 1; }  By using the fold technic, We can implement logical not without operator ! by folding the ‘or’ operation:  /*  * logicalNeg - implement the ! operator, using all of  *              the legal operators except !  *   Examples: logicalNeg(3) = 0, logicalNeg(0) = 1  *   Legal ops: ~ & ^ | + << >>  *   Max ops: 12  *   Rating: 4  */ int logicalNeg(int x) {   x = (x >> 16) | x;   x = (x >> 8) | x;   x = (x >> 4) | x;   x = (x >> 2) | x;   x = (x >> 1) | x;   return ~x & 1; }  Furthermore, we can fill all the bits in the right ( left ) side of the most ( least ) significant bit to 1 by a inverse way (the shift number’s order is reversal):  // All of the bits in the right side of the most significant 1 should set to 1. compare = compare | (compare >> 1); compare = compare | (compare >> 2); compare = compare | (compare >> 4); compare = compare | (compare >> 8); compare = compare | (compare >> 16);  By combining those technic, it is possible to compare two integer number x and y without any arithmetic operator or compare operator:  /*  * isLessOrEqual - if x <= y  then return 1, else return 0  *   Example: isLessOrEqual(4,5) = 1.  *   Legal ops: ! ~ & ^ | + << >>  *   Max ops: 24  *   Rating: 3  */ int isLessOrEqual(int x, int y) {   // Check the sign bits.   // x <= y is possible if any of following condition is satisfied:   // 1. The sign bits of x and y is same.   // 2. The sign bits are not same but the x's sign bit is set to 1   //    (which means x is negative and y is positive).   // If any of those condition is satisfied, the sign_check will be set to 0.   int sign_x = (x >> 31) & 1;   int sign_y = (y >> 31) & 1;   int diff_sign = sign_x ^ sign_y;   int sign_check = diff_sign & (!sign_x);    // Compare x and y.   // For both positive number and negative number, the number which   // contains the most significant 1 will be the greater one.   // Get the different bits between x and y.   int compare = x ^ y;   // Transfer the compare result to the form '00..011..1', which means   // all of the bits in the right of the most significant 1 should set to 1.   compare = compare | (compare >> 1);   compare = compare | (compare >> 2);   compare = compare | (compare >> 4);   compare = compare | (compare >> 8);   compare = compare | (compare >> 16);   // Erase all but the most significant 1.   // For example, the formalized number 00001111 will turn to 00001000   //   (compare >> 1): 00000111   //   (compare &  1): 1   //   (compare >> 1) + (compare & 1): 00001000   // There are two special cases:   // 1. When x == y, compare will be 0 and the result of the expression (compare & 1) will also be 0.   //    Thus the result of the expression (compare >> 1) + (compare & 1) will be 0.   //   // 2. When the sign bit is difference, the result of the express (compare >> 1) will be 0xFFFFFFFFFFFFFFFF.   //    Thus the result of the expression (compare >> 1) + (compare & 1) will be 0.   //    That is, the expression is invalid in this case, but at least it will not interference the sign check.   //    So it still works.   compare = (compare >> 1) + (compare & 1);   // If the most significant bit is contained by x, result will be 0.   // Otherwise it will be 1.   compare = compare & x;    // The x <= y if and only if   // the most significant different bit is contained by x   // and the sign_check is passed.   return !(compare + sign_check); }  Count bits  Consider that we need to find the minimum number of bits required to represent x in two’s component. And all of the operators that allowed to use are: ! ~ & ^ | + << >> .  First at all, the minimum number of bits is only decided by the position of the most significant 1 in the number’s two’s component representation. That is, consider we have x = 00001010 , the most significant 1 is located at the 4th position from the right. So we can represent x by using 5 bits (don’t forget the sign bit).  Wait a minute. How about the negative number? Consider if the x is equals to 11110101 , what is the minimum number of bits to represent it? The answer is also 5 bits. In this situation, we need to find the position of the most significant 0 instead 1. However, it is unnecessary to distinct if the x is negative or positive. Just inverse all of the bits in a negative number, so that we can consider it as a same way to positive numbers.  // Inverse negative numbers x = (x >> 31) ^ x;  It seems hard to find a easy way to find the position of the most significant 1. So we can fill all of the bits located in the right side of the most significant 1 by using the fold technic, so that the problem that find the most significant 1 is converted to the problem that count the number of bits which is set to 1.  // Set the bits in the right side of most significant 1 to 1. x |= x >> 1; x |= x >> 2; x |= x >> 4; x |= x >> 8; x |= x >> 16;  For a number which represented by 2 bits, it is possible to count the number of bits which is set to 1 by using a mask.  mask = 01b; // count the numbers of 1s by adding the two bits. x = (x & mask) + ((x >> 1) & mask);  By using a longer mask, we can count the number of 1s for each 2 bits in a number:  int mask = 0x55555555; // 01010101.... x = (x & mask) + ((x >> 1) & mask)  After this, the number can be considered as a list of 2 bits numbers. Each 2 bits number in the list saves the number of 1 in those 2 bits. For example:  x                              = 0x01001110 mask                           = 0x01010101 x & mask                       = 0x01000100 (x >> 1) & mask                = 0x00000101 (x & mask) + ((x >> 1) & mask) = 0x01001101 2 bits group of x              = 01 00 11 10 2 bits group of result         = 01 00 11 01 (numbers of 1s of the 2 bits group of x)  So we can simply count the 1s for each 4 bits and so far by using a similar way, so that we can implement a function to find the minimum number of bits required to represent x in two’s component.:  /* howManyBits - return the minimum number of bits required to represent x in  *             two's complement  *  Examples: howManyBits(12) = 5  *            howManyBits(298) = 10  *            howManyBits(-5) = 4  *            howManyBits(0)  = 1  *            howManyBits(-1) = 1  *            howManyBits(0x80000000) = 32  *  Legal ops: ! ~ & ^ | + << >>  *  Max ops: 90  *  Rating: 4  */ int howManyBits(int x) {   int mask_2bit = 0x55;   int mask_4bit = 0x33;   int mask_8bit = 0x0f;   int mask_16bit = 0xff;   int mask_32bit = 0xff;    // Inverse negative   x = (x >> 31) ^ x;    // Set the bits in the right of most significant 1 to 1.   x |= x >> 1;   x |= x >> 2;   x |= x >> 4;   x |= x >> 8;   x |= x >> 16;    // Count 1s   // Generate a 2 bit mask 0x55555555(0101....)   mask_2bit += mask_2bit << 8;   mask_2bit += mask_2bit << 16;    // Group each 2 bits to present the sum of 1s in those bits.   x = (x & mask_2bit) + ((x >> 1) & mask_2bit);    // Generate a 4 bit mask 0x33333333(00110011....)   mask_4bit += mask_4bit << 8;   mask_4bit += mask_4bit << 16;    // Group each 4 bits to present the sum of 1s in those bits.   x = (x & mask_4bit) + ((x >> 2) & mask_4bit);    // Generate a 8 bit mask 0x0f0f0f0f(0000111100001111....)   mask_8bit += mask_8bit << 8;   mask_8bit += mask_8bit << 16;    // Group each 8 bits to present the sum of 1s in those bits.   x = (x & mask_8bit) + ((x >> 4) & mask_8bit);    // Generate a 16 bit mask 0x00ff00ff(00000000111111110000000011111111)   mask_16bit += mask_16bit << 16;    // Group each 16 bits to present the sum of 1s in those bits.   x = (x & mask_16bit) + ((x >> 8) & mask_16bit);    // Generate a 32 bit mask 0x00ff00ff(00000000000000001111111111111111)   mask_32bit += mask_32bit << 8;    // Group each 32 bits to present the sum of 1s in those bits.   x = (x & mask_32bit) + ((x >> 16) & mask_32bit);    // Minimum bits to present the number should be numbers of 1s + 1.   return x + 1; } "}]}